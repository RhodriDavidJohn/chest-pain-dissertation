{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23325f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     GridSearchCV,\n",
    "                                     TunedThresholdClassifierCV)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.frozen import FrozenEstimator\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8db312",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not os.getcwd().endswith('chest-pain-dissertation'):\n",
    "    os.chdir('../')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59bf957",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f720b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_size, validation_size, seed):\n",
    "\n",
    "    train_set = int(100*train_size)\n",
    "    val_set = int(100*validation_size)\n",
    "    test_set = int(100*round(1-(train_size+validation_size), 2))\n",
    "\n",
    "    msg = (f\"Splitting data into {train_set}% training set, {val_set}% validation \"\n",
    "           f\"set and {test_set}% testing set...\")\n",
    "    print(msg)\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_set/100, stratify=y, random_state=seed\n",
    "    )\n",
    "    val_size = validation_size/(train_size+validation_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size, stratify=y_train_val, random_state=seed\n",
    "    )\n",
    "\n",
    "    # save the train-test data for model training and evaluation\n",
    "    training_data = X_train.join(y_train)\n",
    "    validation_data = X_val.join(y_val)\n",
    "    testing_data = X_test.join(y_test)\n",
    "\n",
    "    return training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb543d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(X_train, num_cols, disc_cols, cat_cols, removed_features=None):\n",
    "\n",
    "    if removed_features is not None:\n",
    "        for feature in removed_features:\n",
    "            if feature in num_cols:\n",
    "                num_cols.remove(feature)\n",
    "            elif feature in disc_cols:\n",
    "                disc_cols.remove(feature)\n",
    "            elif feature in cat_cols:\n",
    "                cat_cols.remove(feature)\n",
    "            else:\n",
    "                print(f\"Feature {feature} is not valid.\")\n",
    "                raise(ValueError((f\"Feature {feature} is not valid. \"\n",
    "                                  f\"Feature must be in {X_train.columns.values.tolist()}\")))\n",
    "        \n",
    "    invalid_features = list(\n",
    "        set(num_cols+disc_cols+cat_cols) - set(X_train.columns.values.tolist())\n",
    "    )\n",
    "    if len(invalid_features) != 0:\n",
    "        msg = f\"The following features are not in the dataframe: {invalid_features}\"\n",
    "        print(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    impute_and_scale = Pipeline([\n",
    "        (\"numeric_impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"numeric_transformation\", StandardScaler())\n",
    "    ])\n",
    "    binary_and_discrete_impute = Pipeline([\n",
    "        (\"numeric_impute\", SimpleImputer(strategy=\"median\"))\n",
    "    ])\n",
    "    impute_and_one_hot_encode = Pipeline([\n",
    "        (\"categorical_transformation\", OneHotEncoder(handle_unknown='infrequent_if_exist'))\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if len(num_cols)>0:\n",
    "        transformers.append(\n",
    "            (\"numeric_preprocessing\", impute_and_scale, num_cols)\n",
    "        )\n",
    "    if len(disc_cols)>0:\n",
    "        transformers.append(\n",
    "            (\"binary_and_discrete_preprocessing\", binary_and_discrete_impute, disc_cols)\n",
    "        )\n",
    "    if len(cat_cols)>0:\n",
    "        transformers.append(\n",
    "            (\"categorical_preprocessing\", impute_and_one_hot_encode, cat_cols)\n",
    "        )\n",
    "\n",
    "    if len(transformers)>0:\n",
    "        return ColumnTransformer(transformers=transformers)\n",
    "    else:\n",
    "        raise ValueError(\"No transformaers to create pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f834ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X, y, model, params, model_desc, model_name, preprocsessing_pipe, k_fold):\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        # set up the pipeline\n",
    "        pipe = Pipeline([\n",
    "            (\"pre_processing\", preprocsessing_pipe),\n",
    "            (model_desc, model)\n",
    "        ])\n",
    "\n",
    "        # set up grid search object\n",
    "        pipe_cv = GridSearchCV(pipe,\n",
    "                               param_grid=params,\n",
    "                               scoring='roc_auc',\n",
    "                               cv=k_fold,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=0,\n",
    "                               error_score=0.0)\n",
    "\n",
    "        # attempt to fit the model\n",
    "        try:\n",
    "            pipe_cv.fit(X, y)\n",
    "        except Exception as e:\n",
    "            msg = (\"The following error occured \"\n",
    "                f\"while tuning {model_name}: {e}\")\n",
    "            print(msg)\n",
    "            raise(e)\n",
    "        \n",
    "        rounded_score = round(pipe_cv.best_score_, 3)\n",
    "\n",
    "        result = {\n",
    "            'model': pipe_cv.best_estimator_,\n",
    "            'params': pipe_cv.best_params_,\n",
    "            'scores': pipe_cv.best_score_\n",
    "        }\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dac04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_threshold(model, model_name, X_val, y_val):\n",
    "\n",
    "    tuned_model = TunedThresholdClassifierCV(\n",
    "        model,\n",
    "        scoring=\"f1\",\n",
    "        cv=\"prefit\",\n",
    "        refit=False,\n",
    "        store_cv_results=True\n",
    "    )\n",
    "\n",
    "    tuned_model.fit(X_val, y_val)\n",
    "\n",
    "    return tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8afb09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_auc(model: Pipeline, X_val, y_val) -> float:\n",
    "\n",
    "    y_prob = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_prob)\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, X_val, y_val, preprocessor, params, seed):\n",
    "\n",
    "    msg = f\"Training Logistic Regression model tuning the parameters: {params}\"\n",
    "    print(msg)\n",
    "    \n",
    "    model = LogisticRegression(random_state=seed)\n",
    "    \n",
    "    if params != 'None':\n",
    "        trained_model = tune_hyperparameters(X_train, y_train, model, params, \"lreg_model\", \"Logistic Regression\", preprocessor, 5)\n",
    "    else:\n",
    "        trained_model = {}\n",
    "        pipe = Pipeline([\n",
    "            (\"pre_processing\", preprocessor),\n",
    "            ('lreg_model', model)\n",
    "        ])\n",
    "        trained_model['model'] = pipe.fit(X_train, y_train)\n",
    "        \n",
    "    tuned_model = tune_threshold(trained_model['model'], \"Logistic Regression\", X_val, y_val)\n",
    "    trained_model['model'] = tuned_model\n",
    "\n",
    "    trained_model['scores'] = get_validation_auc(tuned_model, X_val, y_val)\n",
    "\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79259e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, X_val, y_val, preprocessor, params, seed):\n",
    "\n",
    "    msg = f\"Training Random Forest model tuning the parameters: {params}\"\n",
    "    print(msg)\n",
    "\n",
    "    model = RandomForestClassifier(criterion=\"gini\", random_state=seed)\n",
    "    \n",
    "    if params != 'None':\n",
    "        trained_model = tune_hyperparameters(X_train, y_train, model, params, \"rfc_model\", \"Random Forest\", preprocessor, 5)\n",
    "    else:\n",
    "        trained_model = {}\n",
    "        pipe = Pipeline([\n",
    "            (\"pre_processing\", preprocessor),\n",
    "            ('rfc_model', model)\n",
    "        ])\n",
    "        trained_model['model'] = pipe.fit(X_train, y_train)\n",
    "        \n",
    "    tuned_model = tune_threshold(trained_model['model'], \"Random Forest\", X_val, y_val)\n",
    "    trained_model['model'] = tuned_model\n",
    "\n",
    "    trained_model['scores'] = get_validation_auc(tuned_model, X_val, y_val)\n",
    "        \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm(X_train, y_train, X_val, y_val, preprocessor, params, seed):\n",
    "\n",
    "    msg = f\"Training LightGBM model tuning the parameters: {params}\"\n",
    "    print(msg)\n",
    "\n",
    "    model = LGBMClassifier(objective='binary', random_state=seed)\n",
    "    \n",
    "    if params != 'None':\n",
    "        trained_model = tune_hyperparameters(X_train, y_train, model, params, \"lgbm_model\", \"LightGBM\", preprocessor, 5)\n",
    "    else:\n",
    "        trained_model = {}\n",
    "        pipe = Pipeline([\n",
    "            (\"pre_processing\", preprocessor),\n",
    "            ('gbm_model', model)\n",
    "        ])\n",
    "        trained_model['model'] = pipe.fit(X_train, y_train)\n",
    "        \n",
    "    tuned_model = tune_threshold(trained_model['model'], \"LightGBM\", X_val, y_val)\n",
    "    trained_model['model'] = tuned_model\n",
    "\n",
    "    trained_model['scores'] = get_validation_auc(tuned_model, X_val, y_val)\n",
    "        \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b72161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, y_train, X_val, y_val, preprocessor, params, seed):\n",
    "\n",
    "    msg = f\"Training XGBoost model tuning the parameters: {params}\"\n",
    "    print(msg)\n",
    "\n",
    "    n_pos = y_train.sum()\n",
    "    n_neg = len(y_train)-n_pos\n",
    "    model = XGBClassifier(objective='binary:logistic', scale_pos_weight=n_neg/n_pos, seed=seed)\n",
    "    \n",
    "    if params != 'None':\n",
    "        trained_model = tune_hyperparameters(X_train, y_train, model, params, \"xgb_model\", \"XGBoost\", preprocessor, 5)\n",
    "    else:\n",
    "        trained_model = {}\n",
    "        pipe = Pipeline([\n",
    "            (\"pre_processing\", preprocessor),\n",
    "            ('xgb_model', model)\n",
    "        ])\n",
    "        trained_model['model'] = pipe.fit(X_train, y_train)\n",
    "        \n",
    "    tuned_model = tune_threshold(trained_model['model'], \"XGBoost\", X_val, y_val)\n",
    "    trained_model['model'] = tuned_model\n",
    "\n",
    "    trained_model['scores'] = get_validation_auc(tuned_model, X_val, y_val)\n",
    "        \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09043f9a",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28461abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean/processed_dataset.csv')\n",
    "X = df.drop(['nhs_number', 'subsequent_mi_30days_diagnosis'], axis=1).copy()\n",
    "y = df['subsequent_mi_30days_diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c253a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, _ = split_data(X, y, train_size=0.6, validation_size=0.2, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(['subsequent_mi_30days_diagnosis'], axis=1).copy()\n",
    "y_train = train_set['subsequent_mi_30days_diagnosis']\n",
    "\n",
    "X_val = val_set.drop(['subsequent_mi_30days_diagnosis'], axis=1).copy()\n",
    "y_val = val_set['subsequent_mi_30days_diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['acute_morbidity_indicator', 'ae_duration_hrs', 'max_tnt_24hr_int',\n",
    "            'min_egfr_24hr_int', 'first_tnt_24hr_int', 'first_egfr_24hr_int',\n",
    "            'mood_and_anxiety_disorders_indicator', 'tnt_egfr_interaction',\n",
    "            'ip_duration_days', 'total_duration_days', 'age', 'tnt_change', 'egfr_change']\n",
    "disc_cols = ['ihd_mi', 'cc_heart_failure', 'cc_myocardial_infarction',\n",
    "             'imd_decile_19', 'qof_diabetes', 'qof_ht', 'ht', 'qof_chd',\n",
    "             'ihd_nonmi', 'af', 'arrhythmia_other', 'stroke', 'hf', 'vasc_dis',\n",
    "             'cardio_other', 'qof_depression', 'qof_mental', 'N_tnt_24hr', 'N_egfr_24hr',\n",
    "             'mi_diagnosis_ae_discharge', 'meds_total', 'meds_antip', 'meds_angio',\n",
    "             'meds_betab', 'meds_total_discharge', 'transfered_dv', 'mi_diagnosis_code',\n",
    "             'chd_diagnosis_code', 'meds_total_more_than_10',\n",
    "             'tnt_rule_in', 'age_threshold', 'ae_target', 'egfr_rule_in']\n",
    "cat_cols = ['ethnicity', 'sex', 'smoking', 'ae_provider', 'ip_provider',\n",
    "            'site_ae', 'site_ip', 'derived_trust_catchment',\n",
    "            'departure_season', 'diagnosis_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a0d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_grid = {'raw': 'None',\n",
    "             'hp1': {'lreg_model__solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']},\n",
    "             'hp2': {'lreg_model__solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n",
    "                     'lreg_model__penalty': [None, 'l1', 'l2', 'elasticnet']},\n",
    "             'hp3': {'lreg_model__solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n",
    "                     'lreg_model__penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "                     'lreg_model__C': np.logspace(-4, 4, 20)},\n",
    "             'hp4': {'lreg_model__solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n",
    "                     'lreg_model__penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "                     'lreg_model__C': np.logspace(-4, 4, 20),\n",
    "                     'lreg_model__max_iter': [500, 1000, 1500, 2000]},\n",
    "             'hp5': {'lreg_model__solver': ['saga', 'liblinear'],\n",
    "                     'lreg_model__penalty': [None, 'l1', 'l2'],\n",
    "                     'lreg_model__C': [0.01, 0.1, 1, 10, 100],\n",
    "                     'lreg_model__max_iter': [750, 1000, 1250, 1500]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b83fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_grid = {'raw': 'None',\n",
    "            'hp1': {'rfc_model__n_estimators': [100, 200, 300, 400]},\n",
    "            'hp2': {'rfc_model__n_estimators': [100, 200, 300, 400],\n",
    "                    'rfc_model__max_features': [0.25, 0.5, 0.75, 1, 'sqrt']},\n",
    "            'hp3': {'rfc_model__n_estimators': [100, 200, 300, 400],\n",
    "                    'rfc_model__max_features': [0.25, 0.5, 0.75, 1, 'sqrt'],\n",
    "                    'rfc_model__max_depth': range(1, 15, 2)},\n",
    "            'hp4': {'rfc_model__n_estimators': [100, 200, 300, 400],\n",
    "                    'rfc_model__max_features': [0.25, 0.5, 0.75, 1, 'sqrt'],\n",
    "                    'rfc_model__max_depth': range(1, 15, 2),\n",
    "                    'rfc_model__min_samples_split': [10, 20, 50, 100]},\n",
    "            'hp5': {'rfc_model__n_estimators': [100, 200, 300, 400],\n",
    "                    'rfc_model__max_depth': range(5, 15, 2),\n",
    "                    'rfc_model__min_samples_split': range(16, 25, 2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_grid = {'raw': 'None',\n",
    "             'hp1': {'lgbm_model__num_leaves': range(11, 72, 20)},\n",
    "             'hp2': {'lgbm_model__num_leaves': range(11, 72, 20),\n",
    "                     'lgbm_model__min_data_in_leaf': [20, 50, 100, 300]},\n",
    "             'hp3': {'lgbm_model__num_leaves': range(11, 72, 20),\n",
    "                     'lgbm_model__min_data_in_leaf': [20, 50, 100, 300],\n",
    "                     'lgbm_model__max_depth': [-1, 1, 5, 10]},\n",
    "             'hp4': {'lgbm_model__num_leaves': range(11, 72, 20),\n",
    "                     'lgbm_model__min_data_in_leaf': [20, 50, 100, 300],\n",
    "                     'lgbm_model__max_depth': [-1, 1, 5, 10],\n",
    "                     'lgbm_model__learning_rate': [0.01, 0.1, 0.2]},\n",
    "             'hp5': {'lgbm_model__num_leaves': [20, 30, 40],\n",
    "                     'lgbm_model__max_depth': [2, 5, 10, 15, 20],\n",
    "                     'lgbm_model__learning_rate': [0.001, 0.01, 0.1],\n",
    "                     'lgbm_model__n_estimators': [100, 300, 500]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = {'raw': 'None',\n",
    "            'hp1': {'xgb_model__n_estimators': [100, 400, 700, 1000]},\n",
    "            'hp2': {'xgb_model__n_estimators': [100, 400, 700, 1000],\n",
    "                    'xgb_model__max_depth': range(2, 11, 2)},\n",
    "            'hp3': {'xgb_model__n_estimators': [100, 400, 700, 1000],\n",
    "                    'xgb_model__max_depth': range(2, 11, 2),\n",
    "                    'xgb_model__xgb_model__gamma': [0, 1, 10, 50, 100]},\n",
    "            'hp4': {'xgb_model__n_estimators': [100, 400, 700, 1000],\n",
    "                    'xgb_model__max_depth': range(2, 11, 2),\n",
    "                    'xgb_model__xgb_model__gamma': [0, 1, 10, 50, 100],\n",
    "                    'xgb_model__subsample': [0.6, 0.8, 1]},\n",
    "            'hp5': {'xgb_model__n_estimators': [500, 750, 1000],\n",
    "                    'xgb_model__eta': [0.01, 0.1, 0.3, 0.5],\n",
    "                    'xgb_model__gamma': [0, 1, 10, 50, 100],\n",
    "                    'xgb_model__max_depth': [2, 4, 6, 8, 10]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f376dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {'Logistic Regression': lreg_grid,\n",
    "              'Random Forest': rfc_grid,\n",
    "              'LightGBM': lgbm_grid,\n",
    "              'XGBoost': xgb_grid}\n",
    "\n",
    "model_dict= {}\n",
    "\n",
    "preprocessor = create_preprocessing_pipeline(X_train, num_cols, disc_cols, cat_cols)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(10, 8))\n",
    "\n",
    "axs_raveled = axs.ravel()\n",
    "\n",
    "\n",
    "for i, (name, params) in enumerate(param_dict.items()):\n",
    "    indv_model_dict = {}\n",
    "    for idx, (key, grid) in enumerate(params.items()):\n",
    "        if name=='Logistic Regression':\n",
    "            trained_model_dict = train_logistic_regression(X_train, y_train, X_val, y_val, preprocessor, grid, seed)\n",
    "        elif name=='Random Forest':\n",
    "            trained_model_dict = train_random_forest(X_train, y_train, X_val, y_val, preprocessor, grid, seed)\n",
    "        elif name=='LightGBM':\n",
    "            trained_model_dict = train_lightgbm(X_train, y_train, X_val, y_val, preprocessor, grid, seed)\n",
    "        else:\n",
    "            trained_model_dict = train_xgboost(X_train, y_train, X_val, y_val, preprocessor, grid, seed)\n",
    "\n",
    "        model = trained_model_dict['model']\n",
    "\n",
    "        indv_model_dict[key] = model\n",
    "\n",
    "        y_scores = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        display = RocCurveDisplay.from_predictions(\n",
    "            y_val,\n",
    "            y_scores,\n",
    "            name=key,\n",
    "            ax=axs_raveled[i],\n",
    "            plot_chance_level=(idx==len(params)-1),\n",
    "            chance_level_kw={'linestyle': ':'}\n",
    "        )\n",
    "    model_dict[name] = indv_model_dict\n",
    "    axs_raveled[i].set_title(name)\n",
    "_ = plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3db52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "axs = ax.ravel()\n",
    "\n",
    "for i, (name, models) in enumerate(model_dict.items()):\n",
    "    hp_list = []\n",
    "    f1_list = []\n",
    "    for key, model in models.items():\n",
    "        y_pred = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        hp_list.append(key)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    df_plot = pd.DataFrame({'hp': hp_list, 'F1 Score': f1_list})\n",
    "    axs[i].barh(df_plot['hp'], df_plot['F1 Score'])\n",
    "    axs[i].set(title=name, ylabel='HP Tuning Group', xlabel='F1 Score')\n",
    "\n",
    "fig.suptitle('F1 scores for each model')\n",
    "\n",
    "_ = plt.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4a3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
