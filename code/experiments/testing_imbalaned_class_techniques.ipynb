{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     GridSearchCV,\n",
    "                                     TunedThresholdClassifierCV)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.frozen import FrozenEstimator\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imb_Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not os.getcwd().endswith('chest-pain-dissertation'):\n",
    "    os.chdir('../')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc1fe1",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, train_size, validation_size, seed):\n",
    "\n",
    "    train_set = int(100*train_size)\n",
    "    val_set = int(100*validation_size)\n",
    "    test_set = int(100*round(1-(train_size+validation_size), 2))\n",
    "\n",
    "    msg = (f\"Splitting data into {train_set}% training set, {val_set}% validation \"\n",
    "           f\"set and {test_set}% testing set...\")\n",
    "    print(msg)\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_set/100, stratify=y, random_state=seed\n",
    "    )\n",
    "    val_size = validation_size/(train_size+validation_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size, stratify=y_train_val, random_state=seed\n",
    "    )\n",
    "\n",
    "    # save the train-test data for model training and evaluation\n",
    "    training_data = X_train.join(y_train)\n",
    "    validation_data = X_val.join(y_val)\n",
    "    testing_data = X_test.join(y_test)\n",
    "\n",
    "    return training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b66f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(X_train, num_cols, disc_cols, cat_cols, removed_features=None):\n",
    "\n",
    "    if removed_features is not None:\n",
    "        for feature in removed_features:\n",
    "            if feature in num_cols:\n",
    "                num_cols.remove(feature)\n",
    "            elif feature in disc_cols:\n",
    "                disc_cols.remove(feature)\n",
    "            elif feature in cat_cols:\n",
    "                cat_cols.remove(feature)\n",
    "            else:\n",
    "                print(f\"Feature {feature} is not valid.\")\n",
    "                raise(ValueError((f\"Feature {feature} is not valid. \"\n",
    "                                  f\"Feature must be in {X_train.columns.values.tolist()}\")))\n",
    "        \n",
    "    invalid_features = list(\n",
    "        set(num_cols+disc_cols+cat_cols) - set(X_train.columns.values.tolist())\n",
    "    )\n",
    "    if len(invalid_features) != 0:\n",
    "        msg = f\"The following features are not in the dataframe: {invalid_features}\"\n",
    "        print(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    impute_and_scale = Pipeline([\n",
    "        (\"numeric_impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"numeric_transformation\", StandardScaler())\n",
    "    ])\n",
    "    binary_and_discrete_impute = Pipeline([\n",
    "        (\"numeric_impute\", SimpleImputer(strategy=\"median\"))\n",
    "    ])\n",
    "    impute_and_one_hot_encode = Pipeline([\n",
    "        (\"categorical_transformation\", OneHotEncoder(handle_unknown='infrequent_if_exist'))\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if len(num_cols)>0:\n",
    "        transformers.append(\n",
    "            (\"numeric_preprocessing\", impute_and_scale, num_cols)\n",
    "        )\n",
    "    if len(disc_cols)>0:\n",
    "        transformers.append(\n",
    "            (\"binary_and_discrete_preprocessing\", binary_and_discrete_impute, disc_cols)\n",
    "        )\n",
    "    if len(cat_cols)>0:\n",
    "        transformers.append(\n",
    "            (\"categorical_preprocessing\", impute_and_one_hot_encode, cat_cols)\n",
    "        )\n",
    "\n",
    "    if len(transformers)>0:\n",
    "        return ColumnTransformer(transformers=transformers)\n",
    "    else:\n",
    "        raise ValueError(\"No transformaers to create pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling_technique(name, preprocessor, model, model_name, seed):\n",
    "        \n",
    "    if name=='Under':\n",
    "        pipeline = imb_Pipeline(steps=[\n",
    "            ('pre_processing', preprocessor),\n",
    "            ('under_sampling', RandomUnderSampler(random_state=seed)),\n",
    "            (model_name, model)\n",
    "        ])\n",
    "    elif name=='Over':\n",
    "        pipeline = imb_Pipeline(steps=[\n",
    "            ('pre_processing', preprocessor),\n",
    "            ('over_sampling', SMOTE(random_state=seed)),\n",
    "            (model_name, model)\n",
    "        ])\n",
    "    elif name=='Under and over':\n",
    "        pipeline = imb_Pipeline(steps=[\n",
    "            ('pre_processing', preprocessor),\n",
    "            ('under_sampling', RandomUnderSampler(random_state=seed)),\n",
    "            ('over_sampling', SMOTE(random_state=seed)),\n",
    "            (model_name, model)\n",
    "        ])\n",
    "    elif name=='Over and under':\n",
    "        pipeline = imb_Pipeline(steps=[\n",
    "            ('pre_processing', preprocessor),\n",
    "            ('over_sampling', SMOTE(random_state=seed)),\n",
    "            ('under_sampling', RandomUnderSampler(random_state=seed)),\n",
    "            (model_name, model)\n",
    "        ])\n",
    "    else: # name=='No resampling'\n",
    "        pipeline = Pipeline([\n",
    "            ('pre_processing', preprocessor),\n",
    "            (model_name, model)\n",
    "        ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X, y, model, params, model_desc, model_name, preprocsessing_pipe, resampling_tech, k_fold, seed):\n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        # set up the pipeline\n",
    "        pipe = resampling_technique(resampling_tech, preprocsessing_pipe, model, model_desc, seed)\n",
    "\n",
    "        # set up grid search object\n",
    "        pipe_cv = GridSearchCV(pipe,\n",
    "                               param_grid=params,\n",
    "                               scoring='roc_auc',\n",
    "                               cv=k_fold,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=0,\n",
    "                               error_score=0.0)\n",
    "\n",
    "        # attempt to fit the model\n",
    "        try:\n",
    "            pipe_cv.fit(X, y)\n",
    "        except Exception as e:\n",
    "            msg = (\"The following error occured \"\n",
    "                f\"while tuning {model_name}: {e}\")\n",
    "            print(msg)\n",
    "            raise(e)\n",
    "\n",
    "        result = {\n",
    "            'model': pipe_cv.best_estimator_,\n",
    "            'params': pipe_cv.best_params_,\n",
    "            'scores': pipe_cv.best_score_\n",
    "        }\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9badc8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_probabilities(model, X_val, y_val):\n",
    "\n",
    "    frozen_clf = FrozenEstimator(model)\n",
    "    model_calib = CalibratedClassifierCV(frozen_clf, method='isotonic')\n",
    "    model_calib.fit(X_val, y_val)\n",
    "\n",
    "    return model_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ee95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_threshold(model, X_val, y_val):\n",
    "\n",
    "    tuned_model = TunedThresholdClassifierCV(\n",
    "        model,\n",
    "        scoring=\"f1\",\n",
    "        cv=\"prefit\",\n",
    "        refit=False,\n",
    "        store_cv_results=True\n",
    "    )\n",
    "\n",
    "    tuned_model.fit(X_val, y_val)\n",
    "\n",
    "    return tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfc399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_auc(model: Pipeline, X_val, y_val) -> float:\n",
    "\n",
    "    y_prob = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_prob)\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f71894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, X_val, y_val, preprocessor, seed, resampling_name):\n",
    "\n",
    "    msg = f\"Training Logistic Regression model using {resampling_name} resampling\"\n",
    "    print(msg)\n",
    "\n",
    "    model = LogisticRegression(random_state=seed)\n",
    "        \n",
    "    params = {\n",
    "        \"lreg_model__solver\": ['saga', 'liblinear'],\n",
    "        \"lreg_model__penalty\": [None, 'l1', 'l2'],\n",
    "        \"lreg_model__C\": [0.01, 0.1, 1, 10, 100],\n",
    "        \"lreg_model__max_iter\": [750, 1000, 1250, 1500]\n",
    "    }\n",
    "    trained_model = tune_hyperparameters(X_train, y_train, model, params, \"lreg_model\", \"Logistic Regression\", preprocessor, resampling_name, 5, seed)\n",
    "        \n",
    "    tuned_model = tune_threshold(trained_model['model'], X_val, y_val)\n",
    "    trained_model['model'] = tuned_model\n",
    "\n",
    "    trained_model['scores'] = get_validation_auc(tuned_model, X_val, y_val)\n",
    "\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, X_val, y_val, preprocessor, seed, resampling_name):\n",
    "\n",
    "    msg = f\"Training Random Forest model using the {resampling_name} resampling\"\n",
    "    print(msg)\n",
    "\n",
    "    model = RandomForestClassifier(criterion=\"gini\", max_features=\"sqrt\",\n",
    "                                   random_state=seed)\n",
    "        \n",
    "    params = {\n",
    "        \"rfc_model__n_estimators\": [100, 200, 300, 400],\n",
    "        \"rfc_model__max_depth\": range(5, 15, 2),\n",
    "        \"rfc_model__min_samples_split\": range(16, 25, 2)\n",
    "    }\n",
    "    trained_model = tune_hyperparameters(X_train, y_train, model, params, \"rfc_model\", \"Random Forest\", preprocessor, resampling_name, 5, seed)\n",
    "        \n",
    "    tuned_model = tune_threshold(trained_model['model'], X_val, y_val)\n",
    "    trained_model['model'] = tuned_model\n",
    "\n",
    "    trained_model['scores'] = get_validation_auc(tuned_model, X_val, y_val)\n",
    "        \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, y_train, X_val, y_val, preprocessor, seed, resampling_name):\n",
    "\n",
    "    msg = f\"Training XGBoost model using the {resampling_name} resampling\"\n",
    "    print(msg)\n",
    "\n",
    "    model = XGBClassifier(objective='binary:logistic', seed=seed)\n",
    "        \n",
    "    params = {\n",
    "        \"xgb_model__n_estimators\": [500, 750, 1000],\n",
    "        \"xgb_model__eta\": [0.01, 0.1, 0.3, 0.5],\n",
    "        \"xgb_model__gamma\": [0, 1, 10, 50, 100],\n",
    "        \"xgb_model__max_depth\": [2, 4, 6, 8, 10]\n",
    "    }\n",
    "    trained_model = tune_hyperparameters(X_train, y_train, model, params, \"xgb_model\", \"XGBoost\", preprocessor, resampling_name, 5, seed)\n",
    "        \n",
    "    tuned_model = tune_threshold(trained_model['model'], X_val, y_val)\n",
    "    trained_model['model'] = tuned_model\n",
    "\n",
    "    trained_model['scores'] = get_validation_auc(tuned_model, X_val, y_val)\n",
    "        \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm(X_train, y_train, X_val, y_val, preprocessor, seed, resampling_name):\n",
    "\n",
    "    msg = f\"Training LightGBM model using the {resampling_name} resampling\"\n",
    "    print(msg)\n",
    "\n",
    "    model = LGBMClassifier(objective='binary', random_state=seed)\n",
    "\n",
    "    params = {\n",
    "        \"lgbm_model__num_leaves\": [20, 30, 40],\n",
    "        \"lgbm_model__max_depth\": [2, 5, 10, 15, 20],\n",
    "        \"lgbm_model__learning_rate\": [0.001, 0.01, 0.1],\n",
    "        \"lgbm_model__n_estimators\": [100, 300, 500]\n",
    "    }\n",
    "    trained_model = tune_hyperparameters(X_train, y_train, model, params, \"lgbm_model\", \"LightGBM\", preprocessor, resampling_name, 5, seed)\n",
    "        \n",
    "    tuned_model = tune_threshold(trained_model['model'], X_val, y_val)\n",
    "    trained_model['model'] = tuned_model\n",
    "\n",
    "    trained_model['scores'] = get_validation_auc(tuned_model, X_val, y_val)\n",
    "        \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2044b6",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8195a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean/processed_dataset.csv')\n",
    "X = df.drop(['nhs_number', 'subsequent_mi_30days_diagnosis'], axis=1).copy()\n",
    "y = df['subsequent_mi_30days_diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, _ = split_data(X, y, train_size=0.6, validation_size=0.2, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a17538",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(['subsequent_mi_30days_diagnosis'], axis=1).copy()\n",
    "y_train = train_set['subsequent_mi_30days_diagnosis']\n",
    "\n",
    "X_val = val_set.drop(['subsequent_mi_30days_diagnosis'], axis=1).copy()\n",
    "y_val = val_set['subsequent_mi_30days_diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['acute_morbidity_indicator', 'ae_duration_hrs', 'max_tnt_24hr_int',\n",
    "            'min_egfr_24hr_int', 'first_tnt_24hr_int', 'first_egfr_24hr_int',\n",
    "            'mood_and_anxiety_disorders_indicator', 'tnt_egfr_interaction',\n",
    "            'ip_duration_days', 'total_duration_days', 'age', 'tnt_change', 'egfr_change']\n",
    "disc_cols = ['ihd_mi', 'cc_heart_failure', 'cc_myocardial_infarction',\n",
    "             'imd_decile_19', 'qof_diabetes', 'qof_ht', 'ht', 'qof_chd',\n",
    "             'ihd_nonmi', 'af', 'arrhythmia_other', 'stroke', 'hf', 'vasc_dis',\n",
    "             'cardio_other', 'qof_depression', 'qof_mental', 'N_tnt_24hr', 'N_egfr_24hr',\n",
    "             'mi_diagnosis_ae_discharge', 'meds_total', 'meds_antip', 'meds_angio',\n",
    "             'meds_betab', 'meds_total_discharge', 'transfered_dv', 'mi_diagnosis_code',\n",
    "             'chd_diagnosis_code', 'meds_total_more_than_10',\n",
    "             'tnt_rule_in', 'age_threshold', 'ae_target', 'egfr_rule_in']\n",
    "cat_cols = ['ethnicity', 'sex', 'smoking', 'ae_provider', 'ip_provider',\n",
    "            'site_ae', 'site_ip', 'derived_trust_catchment',\n",
    "            'departure_season', 'diagnosis_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_names = ['No resampling', 'Under', 'Over', 'Under and over', 'Over and under']\n",
    "\n",
    "preprocessor = create_preprocessing_pipeline(X_train, num_cols, disc_cols, cat_cols)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, squeeze=False, figsize=(10, 10))\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    if i==0:\n",
    "        model_name = 'Logistic Regression'\n",
    "    elif i==1:\n",
    "        model_name = 'Random Forest'\n",
    "    elif i==2:\n",
    "        model_name = 'XGBoost'\n",
    "    else:\n",
    "        model_name = 'LightGBM'\n",
    "\n",
    "    for idx, name in enumerate(sampling_names):\n",
    "            \n",
    "        if model_name=='Logistic Regression':\n",
    "            model_dict = train_logistic_regression(X_train, y_train, X_val, y_val, preprocessor, seed, name)\n",
    "        elif model_name=='Random Forest':\n",
    "            model_dict = train_random_forest(X_train, y_train, X_val, y_val, preprocessor, seed, name)\n",
    "        elif model_name=='XGBoost':\n",
    "            model_dict = train_xgboost(X_train, y_train, X_val, y_val, preprocessor, seed, name)\n",
    "        else:\n",
    "            model_dict = train_lightgbm(X_train, y_train, X_val, y_val, preprocessor, seed, name)\n",
    "\n",
    "        model = model_dict['model']\n",
    "\n",
    "        y_scores = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        display = RocCurveDisplay.from_predictions(\n",
    "            y_val,\n",
    "            y_scores,\n",
    "            name=name,\n",
    "            ax=ax,\n",
    "            plot_chance_level=(idx==len(sampling_names)-1),\n",
    "            chance_level_kw={'linestyle': ':'}\n",
    "        )\n",
    "    ax.set_title(model_name)\n",
    "_ = plt.tight_layout(pad=2.0)\n",
    "plt.savefig('results/experimentation_results/imbalanced_class_techniques_results.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be614eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
